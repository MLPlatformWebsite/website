---
title: About Machine Learning Platform
description: The machine learning platform is part of the Linaro Artificial
  Intelligence Initiative and is the home for open-source software libraries
  (Arm NN, Compute Library and Arm MLIA) that optimise the execution of machine
  learning workloads on Arm-based processors.
permalink: /about/
keywords:
  - Arm NN
  - Compute Library
  - Arm MLIA
layout: flow
jumbotron:
  title: About
  image: /assets/images/content/ml-banner.jpg
flow:
  - row: container_row
    sections:
      - format: text
        text_content:
          text: >-
            ## What is the machine learning platform?


            The machine learning platform is part of the [Linaro](https://www.linaro.org/news/linaro-announces-launch-of-machine-intelligence-initiative/)[ Artificial Intelligence Initiative](https://www.linaro.org/news/linaro-announces-launch-of-machine-intelligence-initiative/) and is the home for open-source software libraries (Arm NN and Compute Library) that optimise the execution of machine learning workloads on Arm-based processors.


            It enables a new era of advanced, ultra-efficient inference at the edge. Specifically designed for machine learning (ML) and neural network (NN) capabilities, the architecture is versatile enough to scale to any device, from the Internet of Things (IoT) to connected cars and servers.


            {:.table}

            |Project|Description|

            |-------|-----------|

            |Arm NN|Arm NN is an inference engine for CPUs, GPUs and NPUs. It bridges the gap between existing NN frameworks and the underlying IP. It enables efficient translation of existing neural network frameworks, such as TensorFlow and Caffe, allowing them to run efficiently – without modification – across Arm Cortex CPUs and Arm Mali GPUs. For more details see: [https://developer.arm.com/products/processors/machine-learning/arm-nn](https://developer.arm.com/products/processors/machine-learning/arm-nn)|

            |Compute Library|The Compute Library contains a comprehensive collection of software functions implemented for the Arm Cortex-A family of CPU processors and the Arm Mali family of GPUs. It is a convenient repository of low-level optimized functions that developers can source individually or use as part of complex pipelines in order to accelerate their algorithms and applications. For more details see: [https://developer.arm.com/technologies/compute-library](https://developer.arm.com/technologies/compute-library)|

            |Arm ML Inference Advisor|The Arm ML Inference Advisor (Arm MLIA) helps AI developers design and optimize neural network models for efficient inference on Arm targets. Arm MLIA enables insights into how the ML model will perform on Arm early in the model development cycle. With the tool, we aim to make the Arm ML IP accessible to developers at all levels of abstraction, with differing knowledge on hardware optimization and machine learning. For more details see: [https://pypi.org/project/mlia/](https://pypi.org/project/mlia/)|


            ## Our Focus


            The aim of the machine learning platform is to provide a home for the development of open-source software that optimises and simplifies the running of machine learning jobs on Arm-based processors.


            We aim to create a thriving community of developers working together to make the machine learning platform a key resource for achieving ultra-efficient inference at the edge.


            ## Join Us!


            If you or your company are interested in participating in this effort, please visit the [Contributing](/contributing/) page. We welcome all feedback and participation in the development of the machine learning platform.


            ## Linaro’s Artificial Intelligence Initiative


            In 2018, Linaro launched the Artificial Intelligence Initiative, kick started by Arm’s donation of Arm NN. The initiative aims to provide the best-in-class Deep Learning performance by leveraging Neural Network acceleration in IP and SoCs from the Arm ecosystem. Currently every IP vendor forks the runtime of each machine learning framework to integrate their hardware blocks and then tune for performance. This leads to a duplication of effort amongst all players, perpetual cost of re-integration for every new rebasing, and overall increased total cost of ownership. To find out more about the initiative and how to get involved, go to [https://www.linaro.org/engineering/artificial-intelligence/](https://www.linaro.org/engineering/artificial-intelligence/).
---
